{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "172edf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1febae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_export_model(trainingjobName: str, epochs: str, version: str):\n",
    "    \n",
    "    import math\n",
    "    import tensorflow as tf\n",
    "    from numpy import array\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, GRU\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from tensorflow.keras.layers import Flatten, Dropout, Activation\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    import numpy as np\n",
    "    print(\"numpy version\")\n",
    "    print(np.__version__)\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from featurestoresdk.feature_store_sdk import FeatureStoreSdk\n",
    "    from modelmetricsdk.model_metrics_sdk import ModelMetricsSdk\n",
    "    \n",
    "    fs_sdk = FeatureStoreSdk()\n",
    "    mm_sdk = ModelMetricsSdk()\n",
    "    \n",
    "\n",
    "    df_new_analytics = fs_sdk.get_features(trainingjobName, ['DRB.UEThpDl', 'RRU.PrbUsedDl', 'PEE.AvgPowerg', 'Viavi.Cell.Name'])\n",
    "    print(\"Dataframe:\")\n",
    "    print(df_new_analytics)\n",
    "\n",
    "\n",
    "    target = df_new_analytics['DRB_UEThpDl']\n",
    "    features = df_new_analytics[['RRU_PrbUsedDl', 'PEE_AvgPowerg']]\n",
    "\n",
    "\n",
    "    scaler_features = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_target = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    features_scaled = scaler_features.fit_transform(features)\n",
    "    target_scaled = scaler_target.fit_transform(target.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "    df_scaled = np.hstack((features_scaled, target_scaled))\n",
    "\n",
    "    def create_sequences(data, lookback):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - lookback):\n",
    "            X.append(data[i:(i + lookback), :])\n",
    "            y.append(data[i + lookback, -1]) \n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    sequence_length = 30\n",
    "    X, y = create_sequences(df_scaled, sequence_length)\n",
    "\n",
    "    split_index = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.001,\n",
    "        decay_steps=10000,\n",
    "        decay_rate=0.96\n",
    "    )\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler), loss='mse', metrics=['mae'])\n",
    "\n",
    "    historyFinal = model.fit(X_train, y_train, epochs=int(epochs), batch_size=128, verbose=1, validation_split=0.2)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    y_test_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "    y_pred_rescaled = scaler_target.inverse_transform(y_pred)\n",
    "\n",
    "    mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "    rmse = math.sqrt(mse)\n",
    "\n",
    "    print(f'Overall MSE: {mse}')\n",
    "    print(f'Overall RMSE: {rmse}')\n",
    "\n",
    "\n",
    "    cell_performance = {}\n",
    "    cell_names = df_new_analytics['Viavi_Cell_Name'].iloc[split_index:]\n",
    "    for cell in df_new_analytics['Viavi_Cell_Name'].unique():\n",
    "        cell_mask = cell_names == cell\n",
    "        cell_y_test = y_test_rescaled[cell_mask]\n",
    "        cell_y_pred = y_pred_rescaled[cell_mask]\n",
    "        cell_mse = mean_squared_error(cell_y_test, cell_y_pred)\n",
    "        cell_rmse = math.sqrt(cell_mse)\n",
    "        cell_performance[cell] = {'MSE': cell_mse, 'RMSE': cell_rmse}\n",
    "        print(f'Cell {cell} - MSE: {cell_mse}, RMSE: {cell_rmse}')\n",
    "\n",
    "    model.save(\"./\")\n",
    "    \n",
    "\n",
    "    data = {}\n",
    "    data['metrics'] = []\n",
    "    data['metrics'].append({'Overall_MSE': str(mse), 'Overall_RMSE': str(rmse)})\n",
    "    for cell, metrics in cell_performance.items():\n",
    "        data['metrics'].append({f'{cell}_MSE': str(metrics['MSE']), f'{cell}_RMSE': str(metrics['RMSE'])})\n",
    "\n",
    "    mm_sdk.upload_metrics(data, trainingjobName, version)\n",
    "    mm_sdk.upload_model(\"./\", trainingjobName, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b2b61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"traininghost/pipelineimage:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bea80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_export(trainingjobName: str, epochs: str, version: str):\n",
    "    trainOp = components.func_to_container_op(train_export_model, base_image=BASE_IMAGE)(trainingjobName, epochs,version)\n",
    "    # Below line to disable caching of pipeline step\n",
    "    trainOp.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    trainOp.container.set_image_pull_policy(\"IfNotPresent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37e6478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"es Pipeline\",\n",
    "    description=\"es\",\n",
    ")\n",
    "def super_model_pipeline( \n",
    "    trainingjob_name: str, epochs: str, version: str):\n",
    "    \n",
    "    train_and_export(trainingjob_name, epochs, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48993fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = super_model_pipeline\n",
    "file_name = \"es_model_pipeline\"\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd311c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "pipeline_name=\"es Pipeline\"\n",
    "pipeline_file = file_name+'.zip'\n",
    "requests.post(\"http://tm.traininghost:32002/pipelines/es/upload\".format(pipeline_name), files={'file':open(pipeline_file,'rb')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f1347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
